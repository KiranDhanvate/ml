import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score


# Load the dataset
df = pd.read_csv('diabetes.csv')

# Display first few rows
print(df.head())

# Basic info
print("\nDataset Shape:", df.shape)
print("\nMissing values:\n", df.isnull().sum())
print("\nSummary Statistics:\n", df.describe())


# Separate features and target
X = df.drop('Outcome', axis=1)
y = df['Outcome']


# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)




error_rate = []

for k in range(1, 21):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    pred_k = knn.predict(X_test)
    error_rate.append(np.mean(pred_k != y_test))

# Plot error rate vs K
plt.figure(figsize=(8,5))
plt.plot(range(1,21), error_rate, color='blue', linestyle='dashed', marker='o')
plt.title('Error Rate vs K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
plt.show()




# Choose optimal k (say 7 after plotting)
k_opt = 7

knn = KNeighborsClassifier(n_neighbors=k_opt)
knn.fit(X_train, y_train)

# Predict on test data
y_pred = knn.predict(X_test)




# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", cm)

# Compute metrics
accuracy = accuracy_score(y_test, y_pred)
error = 1 - accuracy
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print("\n===== Evaluation Metrics =====")
print(f"Accuracy: {accuracy:.4f}")
print(f"Error Rate: {error:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
